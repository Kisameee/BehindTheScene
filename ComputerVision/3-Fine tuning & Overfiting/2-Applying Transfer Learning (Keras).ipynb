{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "A Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n",
    "\n",
    "<table>\n",
    "    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n",
    "    <tr><td colspan=4 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n",
    "</table>\n",
    "\n",
    "*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n",
    "\n",
    "How does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners.\n",
    "\n",
    "In this notebook, we'll see how to implement transfer learning for a classification model.\n",
    "\n",
    "> **Important**:The base model used in this exercise is large, and training is resource-intensive. Before running the code in this notebook, shut down all other notebooks in this library (In each open notebook other than this one, on the **File** menu, click **Close and Halt**). If you experience and Out-of-Memory (OOM) error when running code in this notebook, shut down this entire library, and then reopen it and open only this notebook.\n",
    "\n",
    "## Using Transfer Learning to Train a CNN\n",
    "\n",
    "First, we'll import the latest version of Keras and prepare to load our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "print('Keras version:',keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "Before we can train the model, we need to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circle', 'square', 'triangle']\n",
      "Getting Data...\n",
      "Preparing training dataset...\n",
      "Found 840 images belonging to 3 classes.\n",
      "Preparing validation dataset...\n",
      "Found 360 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# The images are in a folder named 'shapes/training'\n",
    "training_folder_name = '../data/shapes/training'\n",
    "\n",
    "# The folder contains a subfolder for each class of shape\n",
    "classes = sorted(os.listdir(training_folder_name))\n",
    "print(classes)\n",
    "\n",
    "# Our source images are 128x128, but the base model we're going to use was trained with 224x224 images\n",
    "pretrained_size = (224,224)\n",
    "batch_size = 15\n",
    "\n",
    "print(\"Getting Data...\")\n",
    "datagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n",
    "                             validation_split=0.3) # hold back 30% of the images for validation\n",
    "\n",
    "print(\"Preparing training dataset...\")\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    training_folder_name,\n",
    "    target_size=pretrained_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "print(\"Preparing validation dataset...\")\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    training_folder_name,\n",
    "    target_size=pretrained_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a trained model to use as a base\n",
    "The VGG16 model is an image classifier that was trained on the ImageNet dataset - a huge dataset containing thousands of images of many kinds of object. We'll download the trained model, excluding its top layer, and set its input shape to match our image data.\n",
    "\n",
    "*Note: The **keras.applications** namespace includes multiple base models, some which may perform better for your dataset than others. We've chosen this model because it's fairly lightweight within the limited resources of the Azure Notebooks environment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "#Load the base model, not including its final connected layer, and set the input shape to match our images\n",
    "base_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=train_generator.image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the already trained layers and add a custom output layer for our classes\n",
    "The existing feature extraction layers are already trained, so we just need to add a couple of layers so that the model output is the predictions for our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 75267     \n",
      "=================================================================\n",
      "Total params: 14,789,955\n",
      "Trainable params: 75,267\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras import optimizers\n",
    "\n",
    "# Freeze the already-trained layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create layers for classification of our images\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "prediction_layer = Dense(len(classes), activation='sigmoid')(x) \n",
    "model = Model(inputs=base_model.input, outputs=prediction_layer)\n",
    "\n",
    "# Compile the model\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Now print the full model, which will include the layers of the base model plus the dense layer we added\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "With the layers of the CNN defined, we're ready to train the top layer using our image data. This will take a considerable amount of time on a CPU due to the complexity of the base model, so we'll train the model over only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/hugin/anaconda3/envs/baldur/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      "56/56 [==============================] - 198s 4s/step - loss: 0.0971 - accuracy: 0.9595 - val_loss: 5.2579e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model over 1 epoch using 15-image batches and using the validation holdout dataset for validation\n",
    "num_epochs = 1\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Trained Model\n",
    "Now that we've trained the model, we can use it to predict the class of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions created - ready to use model for inference.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to resize image\n",
    "def resize_image(src_img, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image\n",
    "\n",
    "    # rescale the image so the longest edge is the right size\n",
    "    src_img.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new image of the right shape\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the rescaled image onto the new background\n",
    "    new_image.paste(src_img, (int((size[0] - src_img.size[0]) / 2), int((size[1] - src_img.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image(classifier, image_array):\n",
    "    import numpy as np\n",
    "    \n",
    "    # We need to format the input to match the training data\n",
    "    # The data generator loaded the values as floating point numbers\n",
    "    # and normalized the pixel values, so...\n",
    "    img_features = image_array.astype('float32')\n",
    "    img_features /= 255\n",
    "    \n",
    "    # These are the classes our model can predict\n",
    "    classnames = ['circle', 'square', 'triangle']\n",
    "    \n",
    "    # Predict the class of each input image\n",
    "    predictions = classifier.predict(img_features)\n",
    "    \n",
    "    predicted_classes = []\n",
    "    for prediction in predictions:\n",
    "        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n",
    "        # So get the index of the highest probability\n",
    "        class_idx = np.argmax(prediction)\n",
    "        # And append the corresponding class name to the results\n",
    "        predicted_classes.append(classnames[int(class_idx)])\n",
    "    # Return the predictions\n",
    "    return predicted_classes\n",
    "\n",
    "print(\"Functions created - ready to use model for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAACPCAYAAAASl6Y7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5hkdX3n8ff3nFOX7uqu6bnPwAzihSga8xiWRHniuppoYtw1urp5VuIqIAgqsCBoGEBUFBQvyEUugooYIJjESzDGaIxRs7ohK65GE2cRVGAGhrn39L3qXH77xznVUzN2T/d0170+r+c501WnTp3znfrWOfWtX/3O75hzDhERERGRfua1OwARERERkXZTUSwiIiIifU9FsYiIiIj0PRXFIiIiItL3VBSLiIiISN9TUSwiIiIifU9F8RGY2evN7O+X+Nz3mtndjY5JpJ+Y2XFmNmFmfgu25czsGc3ejki3037Zu/q97lFRfATOuXucc7/f7jhEepWZPWJmL53vcefcY865Iedc3Mq4RPqZ9sv+1e91j4riJTKzoN0xSPtZSvtRE2gfk6XQPtlc2i/7Vz/kXgeOjJltNrMvmtluM9trZjeZ2elm9t26ZZyZnWtmDwEPZfOeY2bfMLN9ZrbTzC6bZ/0vMLP/bWajZvavZvbi1vzPep+ZXWJmj5vZuJk9aGa/Z2YDZnanme03s5+a2TvNbHvdcw75SS5b9qrs9koz+0r2Xtif3d5Ut+y3zexqM/seMAU8zcxWmNmnzWxHFstVrfhpsZuZ2V3AccDfZD/F/mmWlzPN7DHgH83s+GxekD3nDDPbmuX6F2Z2Tt36Xmxm283sYjPbleXijLrHV5vZ35jZmJl9P8vRd38lsHTZgpl91Mwey/brT5jZQJNfkp6hfbJ7ab/sH6a651eoKAayA+VXgEeB44Fjgc/Ns/irgecDzzazYeAfgK8BxwDPAL45x/qPBf4WuApYBbwD+IKZrW3of6QPmdkzgfOA33LODQN/ADwCvAd4ejb9AXDaUazWAz4DPIX0w2EauOmwZd4AnA0Mk75vPgtEpO+B3wR+HzhrKf+nfuGcewPwGPBK59wQ8JfZQ/8JOJE0b4fbBfwXoAycAVxnZifVPb4BWEG6D58J3GxmK7PHbgYms2VO48jviQ8BvwY8jzSnxwLvPsr/Yl/SPtndtF/2B9U983DO9f0EnALsBoLD5p8OfLfuvgN+t+7+qcAP51nne4G7s9uXAHcd9vjXgdPa/X/v9ol0h9wFvBTI1c3/BfDyuvtnA9sPy+Uz6u7fCVw1zzaeB+yvu/9t4H1199cDFWDgsPfGt9r9+nT6RFosvTS7fXyWl6fVPV6bF8zz/L8GLshuv5i0WArqHt8FvADwgRB4Zt1jV82xfz8DMNIP6afXPXYK8Mt2v17dMGmf7P5J+2XvT6jumXPq+f4hi7QZeNQ5Fy1i2W2HPe/ni3jOU4A/NrNX1s3LAd9afIgyF+fcw2Z2IenO+Bwz+zpwEek32PpcPbrYdZrZIHAd8HKg1poxbGa+O3hiSf26n0Kazx1mVpvnHbaMLN68r5uZ/SFpi+Ovkb7Gg8BP6hbZe9h+PAUMAWuB4LB1z7edtdl6f1CXTyP9AJcFaJ/sWdove4vqnjmo+0RqG3CcLa4TuTvseU9f5Prvcs6N1E0l59w1SwlWDuWc+3Pn3AtJd0JH+hPbDtKdt+a4w542RXqArdlQd/ti4JnA851zZeBF2XyrW+bw90EFWFOX37Jz7jlL/T/1EbfIeZhZAfgC8FFgvXNuBPgqh+ZlPrtJf0rfVDdv8zzL7iFt2XpOXT5XuPSnZFkE7ZNdT/tl71PdMwcVxan/Q3rAvsbMSmZWNLPfWcTzvgJsMLMLsxMAhs3s+XMsdzfwSjP7AzPzs/W/uP5EEVkaM3ummf1udmCeIT1oxqT94C7NTtDZBJx/2FN/BPxJlo+Xk/aXqxnO1jNqZqtIW0Dm5ZzbAfw9cK2Zlc3MM7Onm9l/OtLzBICdwNMWuWweKJB9kGatU4saOihrTfwi8F4zGzSzZwFvnGfZBPgkab/IdZD2jzOzufpSymG0T/YE7Ze9T3XPHFQUM7tjvpK039JjwHbgvy/ieePAy7LnPkl6ZuZL5lhuG/Aq4DLSA8c24J3o9W+EAnANaSvCk8A60tf5StKfZ39J+uF412HPu4A0b6PA60n7wNVcDwxk67yf9ISChbyR9MPhp8B+4PPAxqX8h/rMB4F3mdko8N+OtGC2v/1P0uJqP/AnwJePYlvnkZ7s8yTp++Fe0tbEuVwCPAzcb2ZjpCeWPPMottXPtE92P+2XPU51z9ws6/ws0tOyoWDuds519LdUaR0z+xCwwTl3NKMgSINon5S5aL+Uduroil1EpFHM7Flm9huW+m3SoaG+1O64RPqZ9kvpJE0pis3s5ZYO2P6wmW1pxjakNZTL3qFcMkzaf3GS9Kfea4H72hrREimXvUO57I39UnnsDQ3vPmHpgNA/I+1zsh34PnCqc+6nDd2QNJ1y2TuUy96hXPYO5bI3KI+9oxktxb8NPOyc+4Vzrkp6hZRXNWE70nzKZe9QLnuHctk7lMveoDz2iGYUxcdy6EDP27N50n2Uy96hXPYO5bJ3KJe9QXnsEc24ot1cA3b/Sh8NMzub9DKflEql//CsZz2rCaHIYvzgBz/Y45yb63rkymWXUS57wyOPPMKePXvmu/jBgrlUHjvHEfZJUC67io6vvWO+XDajKN7OoVek2QQ8cfhCzrnbgdsBTj75ZPfAAw80IRRZDDOb73KrymWXUS57w8knn3ykhxfMpfLYOY6wT4Jy2VV0fO0d8+WyGd0nvg+cYGZPNbM88DqObiBv6RzKZe9QLnuHctk7lMveoDz2iIa3FDvnIjM7D/g64AN3OOf+vdHbkeZTLnuHctk7lMveoVz2BuWxdzSj+wTOua8CX23GuqW1lMveoVz2DuWydyiXvUF57A26op2IiIiI9D0VxSIiIiLS91QUi4iIiEjfU1EsIiIiIn1PRbGIiIiI9D0VxSIiIiLS91QUi4iIiEjfU1EsIiIiIn1PRbGIiIiI9L2mXNFORPpZBSiA+9VHnLUuCqtt3yLSYAzwcHi0MAwREekSKopFpLFcIa0/LQJiIAQiAMyNtC4OI62FXd1hzqWz9RuZiIgcTkWxiDRJram2ABQBD2yihVuPwALABwKMBCwhLdSHWhaHiIh0BxXFItJYBmnLsDE15SDOMViAOAJ/sHXFaEwCWVeJqAKFAlSqExTycctiEBGR7qGiWEQaLKHWh3dwoMjbz72JcHoF1ZkcMy7fsiicOcDHkoB8UCWXO8Ctt51BZQIKaigWEZHDqCgWkcZyHljWl9g8rrvxPM46/TrCKEfsPa21cbgAz0G1MgU2QxhBodS6EEREpHuoKBaRxnKAGWnf3WnwSnzqjrfzP15/JeTXtTCOAM8Z5mDlipgw2U9uMAbbAWxqXRwiItIVVBSLSGN5kB5ahg7ez8Pdf/Uezjr94xzYu4LA1pAvhTivAnhYkmt4GM4ScAHOn2R0/7HkBsZxNo2Fm6Dxm+svcwy3B2QnMi6FhgMRaYv59uWF9Oi4ljoSiUjL3PKp8xks7yZyeyHciIVrSfsgS9dxUJmpHjbPW9okIu1hydKmHqWWYhFpGc+f4Y57Lub8c25hem9EkpSwZKzdYckShGHEfff9DTMzMwdnOn9J63rjGa9rUFQicjTcEhslrEfbVHvzfyUiHSmwIh47ufm21zG07ieMjv+EpDLQ7rDkKLkEgiBgcnISl4BLPFwSYIm/pElE2mNqssLkxAxGgBEQVhNc4oHzMYLZv2MHJnGJx44ndqXLhWG7Q28KtRSLSOs4MBvAMc21Hz+H8958E3uefJzhwqp2RyZHJR0DOuWlk7pBiHSNKIoIgoBSocTkxPRsL7Z77ryXrVu3Ui6XmZmZwczYvHkzZ7/1LFwEx2w4BhLw/fTL7MzMDMVisY3/k8ZSUSwireXKmJXJ+SE3fuI8zjv744QVFcXdq1YQG+Z0YRSRbhDH6b4auIB3/ekVHHPMMezZs4dSqQRVw48D1pTXsnfvXh7/5RNsueByNm7cSKVS4U/fdTFezqNarfZUQQwqikWklaz+Zo5iHj515/m85axbObA3RxJuIGSM4ZUz6egR1eMxm2xfvDKndGSP9CdWL2sxNhfjTK3FIp0mIsGAOIrJ+zmoQCEqcPG5l1AeKTNSXsnUxDSDxRIuhtLAENWZkOpMSOClQ/UMFAYZ3XcAgCsvvZrYjPd98LL04qVeeo6B7xvOEny/e4f30RFMRNru5tveSmnFNIntozSwBqLVEJfB00l4IiLLEUZVwCMX5MDBli2X89GPXM+qVUv/hW5sbIyPXvNx3nP5VUyOTROGIZ7vkyTdPTKFimIRaT8/mR2ubfXI6oPDtfnj7Y5MRKSrFYMi5tIf6i55x6XkcjkmpiaZmppa8jrXrl6NJY4wDMnn8wwODhCFMb51dwcEFcUi0nYeHrmgwm13nsl4+E94/l68pIDFw+0OTUSkq5kDz8H7rrgKnwAXpcXsipUrlrzOfXv2sHfvXgbyBW696RNcc/WHCQIfb4nDMnYKFcUi0nYWgVGgkKtw+53nM7T6MfaNbyWeLrU7NBGRrhRFEQAHdk2w5aLLmRybolQsEQQBuZzP9PT0ktddLpcp5PIkcczo3lHy/gAXn/+nXV9Vdnn4ItIzHBgDeIxx7cfPYf2x06xZW253VCIiXWtqaoqk4njqcU+nVBymvsuvW8aV6YwEy8Zxc84Y3TvKmjUbln7Z6A6hongBjmR2OuyBI0uyZeaaIiDOprr1LPXKMnIE8+WgkdMcm9TAVEcpGE0vHRqXgVXk/JBb7ziPKj9ud2QicpiwUmVqYhLiCBcnuDghdI2fqgkQJxBVcUCV7NjqKm39/3eLIEj79177kWvZv3sfQ8USgedjDRxT3JyHbx5J4iiXhnnXJVc1bN3toKJ4AYaHuXTCkRa7tSniYIFbK4Jrao9ldW5sEZGFVK2S9navTdJczX6t51ivAd3dq6odRgAP/Fq6cvjALbefw0DpCaozu/HjlYyPT0KwF+dP4Jy6Voi0XDxDLu8xODRA7AIS5xE7j1wcN3zKuxlw4+AmSUiIgBA09N+80uLEEeGcgwgGc4PkCjkqYYXxyXGq1SrAwbpmiRxeNqVjHud8nz27dpEzgwjiqYSJiYmDjYq1+qnD6Z21kMNbfGuM9NXzwfkJsRdRtQozTDLFBOSBXPpY7ScKw9IxPT3mLNZ69Vri7XRIS3/9a96oiTkajrtk5+8WteHaBofs4HBtLqfh2kTaIExCpqYmwCX4FuJTIXDTkDRjmoRkApjBJ5xtbEjUorQgM8Ml8MErP9zybX/0A9cCMDQ01PJtL1d3j53RAknF4QUGAVTjCl7gMZVMkvdyTDDGn//dPewb3Ucw4LNvfB+xF+EsIYoijt/4NLb/7HHOP+MCnjr4DAJyhBMRbighiiPMjMC6d5DrjuUASwvihASX3fKa8qUjyb6bp990PDxG9+5l3cgq8NRe3AieX+G2O8/nnDOuJ6xuYiA5niQYxeW3QaQr4Ym0kssNM5BLIKnwjte9AkafZGzbz1h1zNMbvi1zAb6LiLyAa/7ufgp+4dCri8sRWQD5fJ7qTNjS7U5MTOAXPMIwJMh11+egiuIFeDkjiR1mjiSIqBAy6Y3x7luvoVqexjmHF3gUvDwzhRkSy1qGS/DwvgcZXF3i1r+6iXJ1JW9549tYObSSmIDp6jS+7+Pn/YMtxFkxJ43jcFy05SISYizf+KOoR0xsHrEZSQK5JIdfTfjw+66mEHTft+TOVMCjwm2fuZCzT7+OZLoEbhCLlz6ckIgsjVf713J89J7Pcc5LfosNm47Foib083Xg1S4d7mr9ET1wXX42VwskSdoQtHPnTkoDrf0sqo1qkcvluu5cKX3XWogHXt6YjqepUuGyGy/h+s99jMLmgGDIp+pVqLi0GE5q3SScR94VSMIEr+CxP9zL494jfOo7t/Dez19GQsLgwCB+l32D6kYJCXiO2EXEQeOnxI+Ic1XiXBWXB5ePib0Q83XQbpTacG0eO2aHa1tRDjRcm0gbBNmhzVlAEg1y23d+zDZbRWhB4ycPomxKuysmuKzbohxZFEXcdssnZ4dlayXP8/jETbd15dXtFiyKzewOM9tlZv9WN2+VmX3DzB7K/q7M5puZ3WhmD5vZj83spGYG30iz32YScDOkvfkjiL2IGZtkv7+bK/5yC+HGaUaDPYxPjRFVIgpBgbyfpzpdxXPe7BRXY/L5PDPhNAPlIrnhgCdGt3Mg2c/FX38LV/71ZcQWYlUPpoAYZlj6mIGL8aY3vYl169bx67/+67Pz9u3bB3BCL+USyIb38jCMxHckRYdzjZ8qnocfwUA1wUsqhIRYPiCfKzb1v9dXufTI8lnGY4obb3kzB6bvpxLtaHdkDTFXLqMooteOsb2ub/ZJO3hKhTc4DMEIn/nGj5gYWs8oRaaSBFeZIJ9EeM4ntDKO3JKmyEu7pQWJB55HZAEhQNzcqrgXcpnP59mzZw/lcuuHtSyXy+zZsyc92a/LLKal+E7g5YfN2wJ80zl3AvDN7D7AHwInZNPZwK2NCbNFsvxZHvAhdgkhaaFz82duwplb1rh+NXGYsO2JbVx/13WQB+fB1MQ0RRtY9rqP5PTTT+drX/vaIfOuueYagPGey2WP66tcerXh2kpAGYi5466LWbuxN4ZlmiuXTz75JPTiMbaH9dU+OYcbP38fueERJioJhVIZZwAJvmt9S+Vy9UIu4zgmSZLZ0SZaaXJykiRJ8P3ua9JfsE+xc+6fzOz4w2a/CnhxdvuzwLeBS7L5f+bSrwf3m9mImW10znVXk46lg1pHXpU97OL2u26DIUeVgx/Clng4b2kFcmlgiOLGQR7b+Usenvx/HFPazGChlLZON/G8uxe96EU88sgjh8y77777APZmd3svlz2qv3I5kv7xa13uc/heOlzbW866lcHccxndmzA6tZ2h1WMQD0O0Hmy8jTEv3ly5HB0dhTSHtb/fpoNy6XnpR8cZZ72+lZvtaP21T85lgBvu+RJn/df/zJSbILB0wK7ATRNbd52+1Au5dM5RqVQYHBwkDlvbjcHzPKanp7vyRLul9ileX0t49nddNv9YYFvdctuzeR2vfji0mIgK01SY4rovfoQ9wU72sZvRyVESc8se+Hps7xiPj24n2Vjh9n+8mUlGqdhMOq5xi+3cuRPScrxnctmv+jGX19/6ViYqP2J04ucUvJXYzDMgGYRgd7tDW5Yoiui1Y2w/6qd9Mi4MwfA6rr/3PrbN5JgMSkTmE9Abv+h0Wy5rF+5oR7/egYEBnHPkct03ulajT7Sba+yEOTuVmNnZZvaAmT2we3eHfIDNnkDgCAm59PpLmIwniHMhzhKGh4fx3PKHh/DxGV4xREjIo3se4ao7309EFQrLXnUjdXcupV7P5rKQq3DjJ85j1foDOJuEeBUWD4PNtDu0ZllULrstj32o5/bJiPSKc0Nrj+Heb/8z/sg6qn6RoZVr2h1as3VkLsMwZNOmTUxOTjZ1O3OZnJxk06ZNLd9uIyy1KN5pZhsBsr+7svnbgc11y20CnphrBc65251zJzvnTl67du0Sw2iOmIi97Ka4pnBIH2I/OXh5xKV2nYC064Wf+PiJz5o1a/CLHlff/n72hq0/4K1fvx6yThu9mMt+0p+5LFDIT3LbZy6ktHIbnk2Cy3X9cG1BELCcY2z35bE39dM+WasMIwB/gI/d+yV2z8ATY1NtjKpxui2Xzjnecu7ZDA8PN3U78znvwnMZG+u+CywttSj+MnBadvs04L66+W/MzsZ8AXCg3f1qFq3uinWOhFvvuIWwUMFPAvwkIIjy+HGAHwd4zpsdfm2p/CSgEA4w7JUpFovsqjxJVGh969Yf/dEfAazO7vZGLvtUP+YyHa4NPB7nU3eegT/0A6amH6U60dzRP5ptZGQEeu0Y24f6aZ/ME+JnFzOqmgf+EHd9/TvsmOy+Ybnm0m25zOfzzExWWbdu3cILN9iKFWmjRDtGvliuxQzJdi/wz8AzzWy7mZ0JXAO8zMweAl6W3Qf4KvAL4GHgk8DbmhJ1MxizRfEEY0wOjOEVD/4q4rxkdgLwlnPN8LpW5vGpMSr+NIUNAePshwii6ZgwbPwVaE499VROOeUUHnzwQTZt2sSnP/1ptmzZAlDuqVz2AeUy4wGUMEYAj1s+eSGD5d1EbtcCT+wcc+Vyw4YN0GvH2B6nfTKHj0ceyEP6mTq4knvvf4jJoVUcwKdQ9KCyn0IyDXhETR5xaal6IZe7du2iWMozPt76E47jOCauJMRx3PJtL9diRp84dZ6Hfm+OZR1w7nKDaiuDL37rC3iex8TEBHk/39TN+b5PGIZ4nsdffO8veOfzTiQ/mANr/Bmb995773wP/cw5d3L9jJ7IZQ9TLjPeKCQj4Ergg+fPcMddF3P+W25jYqLdwS3OXLm89dZbcc715jG2R2mfnN+Nn7+PC/741XhelUIpxIVVzCX47Ti7fBF6IZfr1q0jqcDFl17IlZde3dJtb3n3JWkjo4+uaNfVsqHYfvnoL4mSCM9r/suTnWXOzMwMO3bsIFfyiVxrr1Mu0r1GsitdpQ1TgRVnh2tbWd7P2J59JFXAKiTJEAlDuFx3HaRFul5uJTd86R/45YRjZ9Wj4hXwCSm4Lvnm2jXSS/8ZAZVKBa8A5CAoFhmbmGD12rVMZK0FRoIto2C1uskRc8zm9VSiKd591eVpc2uutpyXju5ldEXF2QUhtpCllwWOvYgwqbZk4OmZmRniOGZoKL02+ZSbAq89w6iI9JJrb3gT6zbmmZzedcjBf/njx4jI0Yi9APwBPvO17zAaF5j2B4gNrENbintBbTi0MAy5/L0Xs3bDOswgX2zsMFfOEmaq0xw4cIDIRViXV5VdHn6DGcTERBaRy+VaconCYjE9IahSqZAkCWaOhKQlrdQivSwowE23nMrqNQZWwSPSAU+kDWrDteEPzA7Xlh8e6Yfh2trG8zyq1epscbx7904e2fYIQeCBJdkVB5d3RKyNzlUoFJgJZ9KGxC4/yHZ5+A1mkBARe+nUDlUqJPr2LNIAu8kNwO2fPp+Exwlyk0CSXlddRFrGJy02IvMgKPGxz32F7dOwJ+6+izt0k3w+OyfKwTUfuZrLrriE1etW4/k+k1Mzcw+mvEhB3sdZQpD32bFrB28772188MMf6PqqssvDb7wEd8jYxK3mcCTLequKSMrHsn5sd9z5VsanHscI6bLzPkS6XkCIkRCTtRh7BT711W/w6K79bY6sP8yEFcKkyso1K4iJCJOYkZERvGDpXUSLgwWmpqbYP7afGz5+A6vWrgQPoqj7Rpyop6K4JgGqMMgQ09PTlEZKs10bGi30Q7zYJxcWmJyZZKBcxBKP0uQKVrKW8EB3v6lEOsMwOPCYxnJw991XESfTVFt/gSeRPpcO11agbri23KrZ4dq8cnl2uLa8q9DJw7V1o+JAgcHSANVqyNvfcQHV6gzDIysYGBhc8jqffPJJgkKA7/tpd8/srLsg1/xzsZppwSHZ+kbtNEpgbXk9u3bvwEWOQqHx116uXSraWUK+mGM6miZKYsL0O/TswNcisgxxLht7fBrMY3yySnnI4XfW5dRF+tqNd3+eN7zy91k3EBDkSyQYvks/C2OdFdsQcRIShiHFwmDaleLaq8HBB9933ZLX+eGPfYg4SvDzHlEY4xLSk+y6PGdqKa7npWPq7d22jxXBCCvLq5qymdqlohNLIAcTlXFygwHT09MALTnBT6Tn+UAAxip8CpSHhoGVoKJYpHMMr+Oub97PvjjPmMsRm2E48m663ZH1DN/PUSwOHhwWLTs2Xvq+t/Oeay6nEk2BH1MYDKjG0yQW4ryIUrkIfszQigHGJvczOFxgeGSQ91xzOeTBH/QggGDATy+t0OUFMail+FDZV4TLL7iCm+65galoIn3zNHozdSf6hHGVMAoZKAxywdsuBtKRKICmdd8QERHpBLEX4Nsgn/nadzj1xacwmIsoxBV8F6FvsM0XzkRcdsVl5PN58oM5/uyOu9i6dSvlcpnxqXEwGFk9wsWXXdTuUFtCRXGd2NJRH47xN7MiWcX+qb3khhv/EiWW4OHjzDFSXsnUgSlyU0VWsRZQMSwiIv0hApx5BEGJe7/zA857zUs5sPMRii5kcKjd0fW+3FBAHETkizniOObU0183e42Gqamp2WsoxHE8ewXe2jBvvUjdJ+q4bNSHgBxvPe1cjj/uqc3blqXbmh6bppgMMvHEFCXKTdueiIhIp/FJz3OPAPwcN939BRhew9D649obWJ9wJBSKeRwJnm8EOR/z0v7BpaHBbDys9DFHQpDzu+7SzUdDRXGdJB2Qjep4SMlKTI415zT1+iHfoumYgivyvgvfT3WfxicWEZH+ERDiwcHh2gZXaLg2aRsVxTUx5OMCgcuRG/bxCXjvaz9IMOIx400zvHaYyZm0SLbEw5Ijv3ShHxJ7UXoyHWmXicTSS0hHYwm5UkAyEDNUGOatrzuXEmXyQ+rN0jCzHf4TPGJ8QmKPhk9B7BGbR8U3zHnkYw8/8Yh64YwDEZGmyxHAweHa/NzscG3hyl8j8n1273gQXEJCNsy4V2ljvL3FnLekqVepCqupG5ItjEISSwj8gBVTq5naVyX0I8hBNajgJz5+cuSXrlA9tF9wrQ8xQPmYAfbu2kveL/DU9U9nJasJo5C8TipojCyPjvQAGptH5Hnk48b/5BPbwZZ/V9uuJT3845KISGtc9+d/wZtf8husPe4EsPQzV80NDbbUwa56NBG9W+4vg+/75PwckYvY8uoreOrACRx4fByChMgPifxw4XUkwSGjTNQKYoB94/sYHiyTmyjwppeeTZESg8GgvqI0QYJHbEZshu9cwyc3+0tAWoKnvwjA0o80IiIC4KrGJ//xfvbGA2A5emAY3M7jLXHqUSrDampJNvAJ0s7nVsAqA2z5k3fxhHuMG/72ozz0xM/YtGkTQZxjJpymXC4ThunA2PXjC4/G+xgoFYksIiFmuFRmZnyG6mTIqmxTI5cAAAm+SURBVOHV5KeLvPOcSym7lWlzpgexF+ErJQ1leIRhjLMc1aDx7bee83AGPgkugVKxxOToJFEckvd79wxdEZGmGypCEnDbN+4HL4dPbZRUfU5Kc+idVWOH3/Vm5yfTjvVDx3DRq97JVZ98P6tYxUQ4wcjICGEYYmZEUTQ7jAnA8IohIouI4pBKpUI4ETHgShTDQba85nJyFBhkmPrf2b1e/vrVJg5H0YqEcYVqExpv89kVuRODQlDgwJ4DDBVXMOAv/fKZIiKS8QJwAViCOcAS0itFiDSeiuIFuARyg+lOmZ8uceWZV3Pt7R+hYlWSYkQUpVM+nyeO44NPnPGIo4TSwDArCitZO7yBiSemuOjN72AFqxifGCfOxdmZBSlznn4bagBH1kXcQTg5Q8nLMxVWib3Gv939xEu7UJjHvr37WFFaxXBuKD2VWsdtEZElMxJi0l/jAjwgAgfOAn1USlOoKM7Uxt2bbSHOWhWjYpUceZiBNXF6cY0r33IVo+xhnP3c+7/u5dFHH6VSqRAEB1/OwsQgJB7RWMSWt25hFWspUSYaTSAxVpfXQAAxER7Z2ZwJKqQaYPZldFAeHOaqd72HIO/hWePf7hGWvXMccRJR9AbS1owI5VJEZFkqwEB2jE2otTboo1KaRUVxxg7vupB9Dc3VmnKL2QQUGGA9m1mfbOaKU34D9x8TZpgBEqpUcDhGWAOAcw6r+04bjBy6ndk+xIb28gaZfRmzlzpfKDVtW4fsQF7Wh9gAdScWEVmmgbp+xB5YOkKTPiqlWdSJdTmyn8gdDosNnxwFiuQpcuDAAQDMjEpFYyqKiIiIdDK1FC9D1VXxEx/P8/CTgMALqHUSHlxxsHXYTL2fRERERDqZiuJlyOcPniWXy83/e3n9ciIiIiLSedR9QkRERET6nopiEREREel7KopFREREpO+pKBYRERGRvqeiWERERET6nopiEREREel7KopFREREpO+pKBYRERGRvqeiWERERET6nopiEREREel7KopFREREpO8tWBSb2WYz+5aZbTWzfzezC7L5q8zsG2b2UPZ3ZTbfzOxGM3vYzH5sZic1+z8hi7Nt2zZe8pKXcOKJJ/Kc5zyHG264ofaQr1x2F+WyN8yXxyiKUB67i/bJ3qFc9q/FtBRHwMXOuROBFwDnmtmzgS3AN51zJwDfzO4D/CFwQjadDdza8KhlSYIg4Nprr2Xr1q3cf//93Hzzzfz0pz8F2Ihy2VWUy94wXx6ffPJJUB67ivbJ3qFc9q8Fi2Ln3A7n3P/Nbo8DW4FjgVcBn80W+yzw6uz2q4A/c6n7gREz29jwyOWobdy4kZNOSr/ADg8Pc+KJJ/L4448DjKBcdhXlsjfMl8fR0VFQHruK9sneoVz2r6PqU2xmxwO/CfwLsN45twPSwhlYly12LLCt7mnbs3nSQR555BF++MMf8vznPx8gUC67l3LZG+rzGEURymP30j7ZO5TL/rLootjMhoAvABc658aOtOgc89wc6zvbzB4wswd279692DCkASYmJnjta1/L9ddfT7lcPtKiymWHUy57g/LYO5TL3qFc9p9FFcVmliMtiO9xzn0xm72z9vNA9ndXNn87sLnu6ZuAJw5fp3Pudufcyc65k9euXbvU+OUohWHIa1/7Wl7/+tfzmte8pjY7Ui67j3LZG+bKYxAEKI/dR/tk71Au+9NiRp8w4NPAVufcx+oe+jJwWnb7NOC+uvlvzM7GfAFwoPZzg7SXc44zzzyTE088kYsuuqj+oVGUy66iXPaG+fI4MjICymNX0T7ZO5TL/hUsYpnfAd4A/MTMfpTNuwy4BvhLMzsTeAz44+yxrwKvAB4GpoAzGhqxLNn3vvc97rrrLp773OfyvOc9D4APfOADADuAlymX3UO57A3z5XHDhg3s3LlTeewi2id7h3LZvxYsip1z32Xu/jIAvzfH8g44d5lxSRO88IUvJE3Pr4idc8plF1Eue8N8eXz3u9+N8thdtE/2DuWyf+mKdiIiIiLS91QUi4iIiEjfU1EsIiIiIn1PRbGIiIiI9D0VxSIiIiLS92yeMyxbG4TZOPBgu+NYpDXAnnYHsUiLjfUpzrmGjCSuXDbF0cSpXHa2duyTu4HJRW633bolj6Dj60KUyyNQLptmWblczDjFrfCgc+7kdgexGGb2gGI9IuWywdoYp3LZYO2I0zm3Vq9P4+n4emTK5YKUyyZYbqzqPiEiIiIifU9FsYiIiIj0vU4pim9vdwBHQbF23jaXqltibVec3fL6QPfEqlweWbfECTq+LkSxdt42l6pvYu2IE+1ERERERNqpU1qKRURERETapu1FsZm93MweNLOHzWxLB8Rzh5ntMrN/q5u3ysy+YWYPZX9XZvPNzG7MYv+xmZ3Uwjg3m9m3zGyrmf27mV3Q7lg7KZfdksds+x2Vy07KYxaPcrn0eJTLpcXZUXnMtqFcLi1O5XLheJTLGudc2ybAB34OPA3IA/8KPLvNMb0IOAn4t7p5Hwa2ZLe3AB/Kbr8C+DvAgBcA/9LCODcCJ2W3h4GfAc9uV6ydlstuyWOn5bLT8qhcKpc6viqXyqVy2apcti0JWcCnAF+vu38pcGk7Y8riOP6wN8eDwMa6pDyY3b4NOHWu5doQ833Ay9oVayfmshvz2O5cdmIelUvlst251PFVuVQu+yOX7e4+cSywre7+9mxep1nvnNsBkP1dl83viPjN7HjgN4F/oX2xdsRrsYCOziN0RC475rVYgHK5sI55LRbQ0bnsgDy2Yv2NolwurCNei0Xoy1y2uyi2OeZ103AYbY/fzIaALwAXOufGjrToHPMaGWvbX4tl6IjYOySXHfFaLENHxK9cNkTb4++QPLZi/c3W9viVy4Zpe/zNzGW7i+LtwOa6+5uAJ9oUy5HsNLONANnfXdn8tsZvZjnSN8Y9zrkvtjnWbshlR+Yxi6dTctn212KRlMuFtf21WKSOzGUH5bEV628U5XJhyuUyNDuX7S6Kvw+cYGZPNbM88Drgy22OaS5fBk7Lbp9G2o+lNv+N2RmOLwAO1Jrwm83MDPg0sNU597EOiLUbctlxeYSOy2U35BGUy8VQLpeow/IIyuWSKZdL1p+5bEfn6MM6Sr+C9AzCnwOXd0A89wI7gJD0W8aZwGrgm8BD2d9V2bIG3JzF/hPg5BbG+ULSnwF+DPwom17Rzlg7KZfdksdOzGUn5VG5VC51fFUulUvlslW51BXtRERERKTvtbv7hIiIiIhI26koFhEREZG+p6JYRERERPqeimIRERER6XsqikVERESk76koFhEREZG+p6JYRERERPqeimIRERER6Xv/HzuNMv88ChgJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from random import randint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#get the list of test image files\n",
    "test_folder = '../data/shapes/test'\n",
    "test_image_files = os.listdir(test_folder)\n",
    "\n",
    "# Empty array on which to store the images\n",
    "image_arrays = []\n",
    "\n",
    "size = (224,224)\n",
    "background_color=\"white\"\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get the images and show the predicted classes\n",
    "for file_idx in range(len(test_image_files)):\n",
    "    img = Image.open(os.path.join(test_folder, test_image_files[file_idx]))\n",
    "    \n",
    "    # resize the image so it matches the training set - it  must be the same size as the images on which the model was trained\n",
    "    resized_img = np.array(resize_image(img, size, background_color))\n",
    "                      \n",
    "    # Add the image to the array of images\n",
    "    image_arrays.append(resized_img)\n",
    "\n",
    "# Get predictions from the array of image arrays\n",
    "# Note that the model expects an array of 1 or more images - just like the batches on which it was trained\n",
    "predictions = predict_image(model, np.array(image_arrays))\n",
    "\n",
    "# plot easch image with its corresponding prediction\n",
    "for idx in range(len(predictions)):\n",
    "    a=fig.add_subplot(1,len(predictions),idx+1)\n",
    "    imgplot = plt.imshow(image_arrays[idx])\n",
    "    a.set_title(predictions[idx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (BehindTheScene)",
   "language": "python",
   "name": "pycharm-a7c08466"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
